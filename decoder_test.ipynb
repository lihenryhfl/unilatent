{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TextDecoder:\n\tsize mismatch for decode_prefix.weight: copying a param with shape torch.Size([768, 64]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaption_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextDecoder\n\u001b[0;32m----> 2\u001b[0m text_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mTextDecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/bn/us-aigc-temp/henry/unilatent_weights/gpt/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# slightly hacky -- cannot save wte weights since they are shared with lm_head, so we copy them back here\u001b[39;00m\n\u001b[1;32m      4\u001b[0m text_decoder\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m text_decoder\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:841\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(model_file, variant\u001b[39m=\u001b[39mvariant)\n\u001b[1;32m    839\u001b[0m         model\u001b[39m.\u001b[39m_convert_deprecated_attention_blocks(state_dict)\n\u001b[0;32m--> 841\u001b[0m         model, missing_keys, unexpected_keys, mismatched_keys, error_msgs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m    842\u001b[0m             model,\n\u001b[1;32m    843\u001b[0m             state_dict,\n\u001b[1;32m    844\u001b[0m             model_file,\n\u001b[1;32m    845\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m    846\u001b[0m             ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m    847\u001b[0m         )\n\u001b[1;32m    849\u001b[0m         loading_info \u001b[39m=\u001b[39m {\n\u001b[1;32m    850\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmissing_keys\u001b[39m\u001b[39m\"\u001b[39m: missing_keys,\n\u001b[1;32m    851\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39munexpected_keys\u001b[39m\u001b[39m\"\u001b[39m: unexpected_keys,\n\u001b[1;32m    852\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmismatched_keys\u001b[39m\u001b[39m\"\u001b[39m: mismatched_keys,\n\u001b[1;32m    853\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39merror_msgs\u001b[39m\u001b[39m\"\u001b[39m: error_msgs,\n\u001b[1;32m    854\u001b[0m         }\n\u001b[1;32m    856\u001b[0m \u001b[39mif\u001b[39;00m torch_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(torch_dtype, torch\u001b[39m.\u001b[39mdtype):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:932\u001b[0m, in \u001b[0;36mModelMixin._load_pretrained_model\u001b[0;34m(cls, model, state_dict, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[1;32m    929\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    930\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    931\u001b[0m         )\n\u001b[0;32m--> 932\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unexpected_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    935\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    936\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSome weights of the model checkpoint at \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m were not used when\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m initializing \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00munexpected_keys\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m- This IS expected if you are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m BertForSequenceClassification model).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TextDecoder:\n\tsize mismatch for decode_prefix.weight: copying a param with shape torch.Size([768, 64]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "from caption_decoder import TextDecoder\n",
    "text_decoder = TextDecoder.from_pretrained('/mnt/bn/us-aigc-temp/henry/unilatent_weights/gpt/', device_map=None, low_cpu_mem_usage=False)\n",
    "# slightly hacky -- cannot save wte weights since they are shared with lm_head, so we copy them back here\n",
    "text_decoder.transformer.transformer.wte.weight = text_decoder.transformer.lm_head.weight\n",
    "text_decoder.encode_prefix = torch.nn.Linear(1024, 768)\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "text_tokenizer = GPT2Tokenizer.from_pretrained('/mnt/bn/us-aigc-temp/henry/unilatent_weights/gpt_tokenizer/')\n",
    "text_tokenizer.add_special_tokens({'pad_token': text_tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "cv = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "cp = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\n",
      "  warnings.warn(\"bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\")\n",
      "/home/tiger/.local/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "2024-07-23 12:51:35,448 - PixArt - INFO - Constructing dataset FlexibleInternalDataMS...\n",
      "2024-07-23 12:51:35,449 - PixArt - INFO - T5 max token length: 77\n",
      "2024-07-23 12:51:35,450 - PixArt - INFO - ratio of real user prompt: 0.0\n",
      "2024-07-23 12:51:35,496 - PixArt - INFO - /mnt/bn/us-aigc-temp/henry/test.json data volume: 5000\n",
      "2024-07-23 12:51:35,516 - PixArt - INFO - Dataset FlexibleInternalDataMS constructed. time: 0.07 s, length (use/ori): 5000/5000\n",
      "2024-07-23 12:51:35,517 - PixArt - WARNING - Using valid_num=0 in config file. Available 40 aspect_ratios: ['0.25', '0.26', '0.27', '0.28', '0.32', '0.33', '0.35', '0.4', '0.42', '0.48', '0.5', '0.52', '0.57', '0.6', '0.68', '0.72', '0.78', '0.82', '0.88', '0.94', '1.0', '1.07', '1.13', '1.21', '1.29', '1.38', '1.46', '1.67', '1.75', '2.0', '2.09', '2.4', '2.5', '2.89', '3.0', '3.11', '3.62', '3.75', '3.88', '4.0']\n"
     ]
    }
   ],
   "source": [
    "from diffusion.data.builder import build_dataset, build_dataloader\n",
    "from aspect_ratio_sampler import AspectRatioBatchSampler\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "data_config = {\n",
    "    'type': 'FlexibleInternalDataMS',\n",
    "    'roots': [\n",
    "        '/mnt/bn/us-aigc-temp/henry/coco_2014/val/val2014/',\n",
    "        # '/mnt/bn/aigc-us/zjl/laion-coco-aesthetic/data_max1024/',\n",
    "        # '/mnt/bn/aigc-us/zjl/openimages/data/',\n",
    "        # '/mnt/bn/aigc-us/zjl/sharegpt4v_processed_data/data/'\n",
    "    ],\n",
    "    'json_lst': [\n",
    "        '/mnt/bn/us-aigc-temp/henry/test.json',\n",
    "    ],\n",
    "    'load_vae_feat': False,\n",
    "    'load_t5_feat': False\n",
    "}\n",
    "dataset = build_dataset(\n",
    "    data_config, resolution=512, aspect_ratio_type='ASPECT_RATIO_512',\n",
    "    real_prompt_ratio=0.0, max_length=77,\n",
    ")\n",
    "batch_sampler = AspectRatioBatchSampler(sampler=RandomSampler(dataset), dataset=dataset,\n",
    "                                    batch_size=4, aspect_ratios=dataset.aspect_ratio, drop_last=True,\n",
    "                                    ratio_nums=dataset.ratio_nums, valid_num=0)\n",
    "dataloader = build_dataloader(dataset, batch_sampler=batch_sampler, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(x, device='cpu', dtype=torch.float32):\n",
    "    if x.min() < 0:\n",
    "        assert x.min() >= -1.\n",
    "        x = x * .5 + .5\n",
    "    \n",
    "    processed_list = cp(x, do_rescale=False)['pixel_values']\n",
    "    return torch.tensor(np.stack(processed_list)).to(device).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encode_image(batch[0])\n",
    "z = cv(z).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[50256,    32,   582, 10311,   257,  6769,   319,  1353,   286,   257,\n",
      "          9053,  3526,    13, 50257, 50257],\n",
      "        [50256, 15439,   319,  6729,  3526, 14284,   287,  1633,   351, 12269,\n",
      "           287,   262,  4469,    13, 50257],\n",
      "        [50256,    32,  2042,   290,  2330,  3797,   318,  5586,  2045,   503,\n",
      "           257,  4324,    13, 50257, 50257],\n",
      "        [50256,   464, 14580,   286,   257,  1323,   287,   257,  4038, 10162,\n",
      "         50257, 50257, 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>A man riding a wave on top of a surfboard.<|EOS|><|EOS|>',\n",
       " '<|endoftext|>Person on snowboard jumping in air with mountains in the background.<|EOS|>',\n",
       " '<|endoftext|>A black and white cat is sitting looking out a window.<|EOS|><|EOS|>',\n",
       " '<|endoftext|>The reflection of a bus in a vehicle mirror<|EOS|><|EOS|><|EOS|><|EOS|><|EOS|>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [text_tokenizer.bos_token + txt + text_tokenizer.eos_token for txt in batch[1]]\n",
    "out = text_tokenizer(text, return_tensors='pt', truncation=True, padding=\"longest\")\n",
    "print(out)\n",
    "text_tokenizer.batch_decode(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mask(orig_mask, prefix_len=77):\n",
    "    extra_zeros = torch.zeros(size=(orig_mask.shape[0], 77), dtype=orig_mask.dtype, device=orig_mask.device)\n",
    "    return torch.cat([extra_zeros, orig_mask], axis=1)\n",
    "\n",
    "def pad_ids(input_ids, prefix_len=77):\n",
    "    extra_zeros = torch.zeros(len(input_ids), prefix_len, dtype=torch.int64, device=input_ids.device)\n",
    "    return torch.cat([extra_zeros, input_ids], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:, :77].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,    32,   582, 10311,   257,  6769,   319,  1353,   286,   257,\n",
       "          9053,  3526,    13, 50257, 50257],\n",
       "        [50256, 15439,   319,  6729,  3526, 14284,   287,  1633,   351, 12269,\n",
       "           287,   262,  4469,    13, 50257],\n",
       "        [50256,    32,  2042,   290,  2330,  3797,   318,  5586,  2045,   503,\n",
       "           257,  4324,    13, 50257, 50257],\n",
       "        [50256,   464, 14580,   286,   257,  1323,   287,   257,  4038, 10162,\n",
       "         50257, 50257, 50257, 50257, 50257]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50258, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_decoder.transformer.transformer.wte.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.1607, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = pad_mask(out['attention_mask'], prefix_len=z.shape[1])\n",
    "# mask = out['attention_mask']\n",
    "llm_out = text_decoder.forward(out['input_ids'], z[:, :77])#, attention_mask=mask)\n",
    "llm_out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3160, -0.2841,  1.7209,  0.2218], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_discrete_loss(logits, input_ids):\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(\n",
    "        input_ids.shape\n",
    "    )\n",
    "    decoder_nll = decoder_nll.mean(dim=-1)\n",
    "    return decoder_nll\n",
    "\n",
    "# token_discrete_loss(llm_out.logits[:, -7:], out['input_ids'])\n",
    "token_discrete_loss(llm_out.logits, pad_ids(out['input_ids'])) - llm_out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 92, 50258])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import UniDiffuserPipeline\n",
    "\n",
    "# pipe = UniDiffuserPipeline.from_pretrained(\"thu-ml/unidiffuser-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pipe.text_decoder.encode_prefix = nn.Identity()\n",
    "# # pipe.text_decoder.decode_prefix = nn.Identity()\n",
    "# pipe.text_decoder.save_pretrained('/mnt/bn/us-aigc-temp/henry/unilatent_weights/gpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.text_tokenizer.save_pretrained('/mnt/bn/us-aigc-temp/henry/unilatent_weights/gpt_tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
