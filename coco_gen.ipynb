{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/home/tiger/.local/lib/python3.9/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\n",
      "  warnings.warn(\"bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import json\n",
    "from unilatent import UniLatentPipeline\n",
    "\n",
    "from data.builder import build_dataset, build_dataloader\n",
    "from aspect_ratio_sampler import AspectRatioBatchSampler\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing dataset FlexibleInternalDataMS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 06:14:56,467 - PixArt - WARNING - Using valid_num=0 in config file. Available 40 aspect_ratios: ['0.25', '0.26', '0.27', '0.28', '0.32', '0.33', '0.35', '0.4', '0.42', '0.48', '0.5', '0.52', '0.57', '0.6', '0.68', '0.72', '0.78', '0.82', '0.88', '0.94', '1.0', '1.07', '1.13', '1.21', '1.29', '1.38', '1.46', '1.67', '1.75', '2.0', '2.09', '2.4', '2.5', '2.89', '3.0', '3.11', '3.62', '3.75', '3.88', '4.0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FlexibleInternalDataMS constructed. time: 42.67 s, length (use/ori): 7591625/7596238\n"
     ]
    }
   ],
   "source": [
    "# data_config = {\n",
    "#     'type': 'FlexibleInternalDataMS',\n",
    "#     'roots': [\n",
    "#         # '/mnt/bn/us-aigc-temp/henry/coco_2014/val/val2014/',\n",
    "#         '/mnt/bn/aigc-us/zjl/laion-coco-aesthetic/data_max1024/',\n",
    "#     ],\n",
    "#     'json_lst': [\n",
    "#         # '/mnt/bn/us-aigc-temp/henry/test.json',\n",
    "#         '/mnt/bn/aigc-us/zjl/laion-coco-aesthetic/data_max1024/meta_data_coco_edited.json',\n",
    "#     ],\n",
    "#     'load_vae_feat': False,\n",
    "#     'load_t5_feat': False\n",
    "# }\n",
    "# dataset = build_dataset(\n",
    "#     data_config, resolution=512, aspect_ratio_type='ASPECT_RATIO_512',\n",
    "#     real_prompt_ratio=0.0, max_length=77, return_image_id=True\n",
    "# )\n",
    "# batch_sampler = AspectRatioBatchSampler(sampler=RandomSampler(dataset), dataset=dataset,\n",
    "#                                     batch_size=1, aspect_ratios=dataset.aspect_ratio, drop_last=True,\n",
    "#                                     ratio_nums=dataset.ratio_nums, valid_num=0)\n",
    "# dataloader = build_dataloader(dataset, batch_sampler=batch_sampler, num_workers=10)\n",
    "\n",
    "accelerator = Accelerator(\n",
    "        mixed_precision='fp16',\n",
    "    )\n",
    "\n",
    "data_config = {\n",
    "    'type': 'FlexibleInternalDataMS',\n",
    "    'roots': [\n",
    "        '/mnt/bn/aigc-us/zjl/laion-coco-aesthetic/data_max1024/',\n",
    "    ],\n",
    "    'json_lst': [\n",
    "        '/mnt/bn/aigc-us/zjl/laion-coco-aesthetic/data_max1024/meta_data_coco_edited.json',\n",
    "    ],\n",
    "    'load_vae_feat': False,\n",
    "    'load_t5_feat': False\n",
    "}\n",
    "dataset = build_dataset(\n",
    "    data_config, resolution=512, aspect_ratio_type='ASPECT_RATIO_512',\n",
    "    real_prompt_ratio=0.0, max_length=77,\n",
    ")\n",
    "batch_sampler = AspectRatioBatchSampler(sampler=RandomSampler(dataset), dataset=dataset,\n",
    "                                    batch_size=1, aspect_ratios=dataset.aspect_ratio, drop_last=True,\n",
    "                                    ratio_nums=dataset.ratio_nums, valid_num=0)\n",
    "dataloader = build_dataloader(dataset, batch_sampler=batch_sampler, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(accelerator, pipe):\n",
    "    (\n",
    "        pipe.transformer,\n",
    "        pipe.text_encoder, \n",
    "        pipe.text_encoder_2,\n",
    "        pipe.clip_image_encoder,\n",
    "        pipe.text_decoder,\n",
    "        pipe.vae\n",
    "    ) = accelerator.prepare(\n",
    "        pipe.transformer,\n",
    "        pipe.text_encoder, \n",
    "        pipe.text_encoder_2,\n",
    "        pipe.clip_image_encoder,\n",
    "        pipe.text_decoder,\n",
    "        pipe.vae\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def dift_sampler(batch, pipe, index, block_num):\n",
    "    index_ = torch.zeros(size=(1,), dtype=torch.long) + index\n",
    "    embeds, pooled_embeds = pipe.dift_features(batch[0][:1], index_, return_layer=block_num)\n",
    "    embeds = torch.cat([embeds, pooled_embeds], axis=1)\n",
    "    decoded_tokens = pipe.text_decoder.generate_captions(embeds, \n",
    "                        eos_token_id=pipe.decoder_tokenizer.eos_token_id, device=accelerator.device)[0]\n",
    "    decoded_text = pipe.decoder_tokenizer.batch_decode(decoded_tokens)[0]\n",
    "    return decoded_text\n",
    "\n",
    "def clip_sampler(batch, pipe):\n",
    "    embeds, pooled_embeds = pipe.encode_image(batch[0][:1])\n",
    "    embeds = torch.cat([embeds, pooled_embeds], axis=1)\n",
    "    decoded_tokens = pipe.text_decoder.generate_captions(embeds, \n",
    "                        eos_token_id=pipe.decoder_tokenizer.eos_token_id, device=accelerator.device)[0]\n",
    "    decoded_text = pipe.decoder_tokenizer.batch_decode(decoded_tokens)[0]\n",
    "    return decoded_text\n",
    "\n",
    "def generate_captions(pipe, dataloader, save_path, sampler, sampler_kwargs={}):\n",
    "    json_list = []\n",
    "    progbar = tqdm(dataloader)\n",
    "    for i, batch in enumerate(progbar):\n",
    "        with torch.no_grad():\n",
    "            decoded_text = sampler(batch, pipe, **sampler_kwargs)\n",
    "        \n",
    "        caption = decoded_text.strip('!').replace('<|endoftext|>', '').replace('<|EOS|>', '').strip()\n",
    "        image_id = batch[-1]['image_id'].item() if 'image_id' in batch[-1] else 0\n",
    "        json_list.append({'image_id': image_id, 'caption': caption})\n",
    "\n",
    "        progbar.set_description(f\"Image: {i:05d} | Predicted: {caption} | True: {batch[1][0]}\")\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            with open(save_path, 'w') as f:\n",
    "                test = json.dump(json_list, f)\n",
    "\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline from /mnt/bn/us-aigc-temp/henry/data/dift/index_500_block_6/epoch_0_step_49999/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bc8168e01d4a4bb1dd776d9d629b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TextDecoder were not initialized from the model checkpoint at /mnt/bn/us-aigc-temp/henry/data/dift/index_500_block_6/epoch_0_step_49999/text_decoder and are newly initialized: ['pooled_image_embedder.weight', 'transformer.lm_head.weight', 'image_embedder.bias', 'image_embedder.weight', 'pooled_image_embedder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sampler for index_500_block_6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7591625 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2803.8152, device='cuda:0') tensor(0.0344, device='cuda:0') tensor(4.4712, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00000 | Predicted: a man in a suit and tie standing outside. | True: Two red and white quartzite point standing on top of a wooden slab.:   0%|          | 1/7591625 [00:05<12546:25:04,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2852.0764, device='cuda:0') tensor(0.0228, device='cuda:0') tensor(4.5482, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00001 | Predicted: a man in a suit and tie standing outside. | True: Food is displayed on wooden crates at an event.:   0%|          | 2/7591625 [00:06<6465:52:09,  3.07s/it]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2801.2339, device='cuda:0') tensor(0.0035, device='cuda:0') tensor(4.4672, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00002 | Predicted: a man in a suit and tie standing outside. | True: The head and shoulders of an orange bearded lizard.:   0%|          | 3/7591625 [00:08<4506:16:10,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2920.0674, device='cuda:0') tensor(0.0568, device='cuda:0') tensor(4.6563, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00003 | Predicted: a man in man in man in a suit. | True: Three red bags sitting on the steps.:   0%|          | 4/7591625 [00:08<3427:19:52,  1.63s/it]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2806.8984, device='cuda:0') tensor(0.0176, device='cuda:0') tensor(4.4762, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00004 | Predicted: a man is smiling for the camera. | True: The kitchen has an island with wine racks.:   0%|          | 5/7591625 [00:09<2975:49:41,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2868.6042, device='cuda:0') tensor(0.0095, device='cuda:0') tensor(4.5746, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00005 | Predicted: a man in a suit and tie standing outside. | True: Two men on the sidelines with one pointing at something.:   0%|          | 6/7591625 [00:10<2666:40:49,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2797.3787, device='cuda:0') tensor(0.0138, device='cuda:0') tensor(4.4610, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00006 | Predicted: a man in a suit and tie standing outside. | True: The bracelet is made with lava and green jade.:   0%|          | 7/7591625 [00:11<2465:34:00,  1.17s/it]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2931.7610, device='cuda:0') tensor(0.0046, device='cuda:0') tensor(4.6753, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00007 | Predicted: a man in a suit and tie standing outside. | True: Two people looking at the Belugas in an aquarium.:   0%|          | 8/7591625 [00:12<2338:12:24,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2862.6892, device='cuda:0') tensor(0.0167, device='cuda:0') tensor(4.5652, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00008 | Predicted: a man is smiling for the camera. | True: A group of military tanks sitting on top of a table.:   0%|          | 9/7591625 [00:13<2214:54:50,  1.05s/it]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2929.7568, device='cuda:0') tensor(0.0413, device='cuda:0') tensor(4.6720, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00009 | Predicted: a man is smiling for the camera. | True: The calendar for 2016 year with colorful circles.:   0%|          | 10/7591625 [00:14<2207:03:00,  1.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2902.4514, device='cuda:0') tensor(0.0103, device='cuda:0') tensor(4.6286, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00010 | Predicted: a man in a suit and tie standing outside. | True: A gray wedding dress with white lace and tulle.:   0%|          | 11/7591625 [00:15<2160:52:03,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2823.6553, device='cuda:0') tensor(0.0370, device='cuda:0') tensor(4.5028, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00011 | Predicted: a man in a suit and tie standing outside. | True: The screen protector for Samsung Galaxy Core Prime.:   0%|          | 12/7591625 [00:16<2166:00:07,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2865.7021, device='cuda:0') tensor(0.0147, device='cuda:0') tensor(4.5700, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00012 | Predicted: a man is smiling for the camera. | True: Two dirt bikes parked on the side of a road.:   0%|          | 13/7591625 [00:17<2131:35:21,  1.01s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2820.8591, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.4985, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00013 | Predicted: a man in a suit and tie standing outside. | True: The child's crocheted shoes are brightly colored.:   0%|          | 14/7591625 [00:18<2126:44:46,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2857.0354, device='cuda:0') tensor(0.0277, device='cuda:0') tensor(4.5561, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00014 | Predicted: a man in a suit and tie standing outside. | True: A baby is wrapped in a blue bear blanket.:   0%|          | 15/7591625 [00:19<2136:22:48,  1.01s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2822.0171, device='cuda:0') tensor(0.0385, device='cuda:0') tensor(4.5002, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00015 | Predicted: the man is smiling for the camera. | True: various bearing and seal kits for the rollers:   0%|          | 16/7591625 [00:20<2148:00:39,  1.02s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2788.3735, device='cuda:0') tensor(0.0346, device='cuda:0') tensor(4.4465, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00016 | Predicted: a man in a suit and tie standing outside. | True: The words for salsa are in red and white on a women's tank top.:   0%|          | 17/7591625 [00:22<2250:37:37,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2812.1951, device='cuda:0') tensor(0.0696, device='cuda:0') tensor(4.4841, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00017 | Predicted: a man in a suit and tie standing in front of a white background. | True: An Apple mouse is shown on the white surface.:   0%|          | 18/7591625 [00:23<2241:01:59,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2832.5264, device='cuda:0') tensor(0.0214, device='cuda:0') tensor(4.5170, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00018 | Predicted: a man in a suit and tie standing outside. | True: A police officer points at the window in his office.:   0%|          | 19/7591625 [00:24<2221:15:10,  1.05s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2775.8013, device='cuda:0') tensor(0.0250, device='cuda:0') tensor(4.4266, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00019 | Predicted: a man in a suit and tie standing outside. | True: The model is wearing an orange and green dress.:   0%|          | 20/7591625 [00:25<2204:14:44,  1.05s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2839.4426, device='cuda:0') tensor(0.0317, device='cuda:0') tensor(4.5280, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00020 | Predicted: a man in a suit and tie standing outside. | True: Two children sitting on the floor playing with toys.:   0%|          | 21/7591625 [00:26<2197:51:13,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2849.9678, device='cuda:0') tensor(0.0138, device='cuda:0') tensor(4.5449, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00021 | Predicted: a man in a suit and tie standing outside. | True: The refrigerator is decorated with an artistic flower design.:   0%|          | 22/7591625 [00:27<2157:13:30,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2857.4897, device='cuda:0') tensor(0.0241, device='cuda:0') tensor(4.5568, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00022 | Predicted: the man is smiling for the camera. | True: Boats are docked on the river in front of buildings.:   0%|          | 23/7591625 [00:28<2158:44:08,  1.02s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2867.0889, device='cuda:0') tensor(0.0311, device='cuda:0') tensor(4.5721, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00023 | Predicted: a man is smiling for the camera. | True: The well - connected man is being shown with his dog.:   0%|          | 24/7591625 [00:29<2162:22:30,  1.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2834.8398, device='cuda:0') tensor(0.0351, device='cuda:0') tensor(4.5206, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00024 | Predicted: a man in a suit and tie standing in front of a white wall. | True: Two palm trees stand in front of the mountains.:   0%|          | 25/7591625 [00:30<2166:42:14,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2826.8181, device='cuda:0') tensor(0.0097, device='cuda:0') tensor(4.5080, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00025 | Predicted: a man in a suit and tie standing outside. | True: The young boy is standing in front of a door.:   0%|          | 26/7591625 [00:31<2146:48:27,  1.02s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2948.4783, device='cuda:0') tensor(0.0550, device='cuda:0') tensor(4.7017, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00026 | Predicted: a man in man in man in a suit. | True: An outdoor fire pit with the words Amazing London Rooftops.:   0%|          | 27/7591625 [00:32<1999:12:22,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2856.2441, device='cuda:0') tensor(0.0329, device='cuda:0') tensor(4.5548, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00027 | Predicted: a man in a suit and tie standing outside. | True: The silhouette of a man with a goat on his head.:   0%|          | 28/7591625 [00:33<2053:41:35,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2815.9836, device='cuda:0') tensor(0.0249, device='cuda:0') tensor(4.4906, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00028 | Predicted: a man in a suit and tie standing outside. | True: an image of the beautiful actress in yellow:   0%|          | 29/7591625 [00:34<2087:57:08,  1.01it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOA INSIDE CAPTION DECODER tensor(2805.9497, device='cuda:0') tensor(0.0529, device='cuda:0') tensor(4.4744, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: 00029 | Predicted: a man in a white shirt and black pants. | True: A glass table with metal legs and a round top.:   0%|          | 30/7591625 [00:36<2539:50:53,  1.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning sampler for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m sampler_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m: index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_num\u001b[39m\u001b[38;5;124m'\u001b[39m: block_num}\n\u001b[0;32m---> 15\u001b[0m \u001b[43mgenerate_captions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdift_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m, in \u001b[0;36mgenerate_captions\u001b[0;34m(pipe, dataloader, save_path, sampler, sampler_kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progbar):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 42\u001b[0m         decoded_text \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampler_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     caption \u001b[38;5;241m=\u001b[39m decoded_text\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|EOS|>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     45\u001b[0m     image_id \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mdift_sampler\u001b[0;34m(batch, pipe, index, block_num)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdift_sampler\u001b[39m(batch, pipe, index, block_num):\n\u001b[1;32m     21\u001b[0m     index_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;241m+\u001b[39m index\n\u001b[0;32m---> 22\u001b[0m     embeds, pooled_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdift_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([embeds, pooled_embeds], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     decoded_tokens \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mtext_decoder\u001b[38;5;241m.\u001b[39mgenerate_captions(embeds, \n\u001b[1;32m     25\u001b[0m                         eos_token_id\u001b[38;5;241m=\u001b[39mpipe\u001b[38;5;241m.\u001b[39mdecoder_tokenizer\u001b[38;5;241m.\u001b[39meos_token_id, device\u001b[38;5;241m=\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdevice)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/bn/us-aigc-temp/henry/unilatent/unilatent.py:1148\u001b[0m, in \u001b[0;36mUniLatentPipeline.dift_features\u001b[0;34m(self, image, index, return_layer)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdift_features\u001b[39m(\u001b[39mself\u001b[39m, image, index, return_layer\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m):\n\u001b[1;32m   1146\u001b[0m     prompt_embeds, pooled_prompt_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_text(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m     _, (_, hidden), _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_to_denoiser(\n\u001b[1;32m   1149\u001b[0m         image,\n\u001b[1;32m   1150\u001b[0m         prompt_embeds,\n\u001b[1;32m   1151\u001b[0m         pooled_prompt_embeds,\n\u001b[1;32m   1152\u001b[0m         index,\n\u001b[1;32m   1153\u001b[0m         return_layer\u001b[39m=\u001b[39;49mreturn_layer,\n\u001b[1;32m   1154\u001b[0m         apply_embedding\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1155\u001b[0m         )\n\u001b[1;32m   1157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrelength\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1158\u001b[0m         hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdift_relength(hidden)\n",
      "File \u001b[0;32m/mnt/bn/us-aigc-temp/henry/unilatent/unilatent.py:1100\u001b[0m, in \u001b[0;36mUniLatentPipeline.embed_to_denoiser\u001b[0;34m(self, image, prompt_embeds, pooled_prompt_embeds, index, return_layer, apply_embedding)\u001b[0m\n\u001b[1;32m   1097\u001b[0m pooled_prompt_embeds \u001b[39m=\u001b[39m pooled_prompt_embeds\u001b[39m.\u001b[39mreshape(B, C)\n\u001b[1;32m   1099\u001b[0m noisy_latent, timestep, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scale_noise(latent, index, noise)\n\u001b[0;32m-> 1100\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1101\u001b[0m             hidden_states\u001b[39m=\u001b[39;49mnoisy_latent,\n\u001b[1;32m   1102\u001b[0m             timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m   1103\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39;49mprompt_embeds,\n\u001b[1;32m   1104\u001b[0m             pooled_projections\u001b[39m=\u001b[39;49mpooled_prompt_embeds,\n\u001b[1;32m   1105\u001b[0m             joint_attention_kwargs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1106\u001b[0m             return_layer\u001b[39m=\u001b[39;49mreturn_layer\n\u001b[1;32m   1107\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m return_layer:\n\u001b[1;32m   1110\u001b[0m     \u001b[39mreturn\u001b[39;00m model_output\u001b[39m.\u001b[39msample, model_output\u001b[39m.\u001b[39mhidden, target\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/bn/us-aigc-temp/henry/unilatent/transformer.py:345\u001b[0m, in \u001b[0;36mSD3Transformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, block_controlnet_hidden_states, joint_attention_kwargs, return_dict, return_layer)\u001b[0m\n\u001b[1;32m    336\u001b[0m     encoder_hidden_states, hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    337\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    338\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     encoder_hidden_states, hidden_states \u001b[39m=\u001b[39m block(\n\u001b[1;32m    346\u001b[0m         hidden_states\u001b[39m=\u001b[39;49mhidden_states, encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states, temb\u001b[39m=\u001b[39;49mtemb\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m return_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m index_block \u001b[39m==\u001b[39m return_layer:\n\u001b[1;32m    350\u001b[0m     return_hidden \u001b[39m=\u001b[39m encoder_hidden_states, hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/diffusers/models/attention.py:206\u001b[0m, in \u001b[0;36mJointTransformerBlock.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, temb)\u001b[0m\n\u001b[1;32m    202\u001b[0m         context_ff_output \u001b[39m=\u001b[39m _chunked_feed_forward(\n\u001b[1;32m    203\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff_context, norm_encoder_hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_size\n\u001b[1;32m    204\u001b[0m         )\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         context_ff_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff_context(norm_encoder_hidden_states)\n\u001b[1;32m    207\u001b[0m     encoder_hidden_states \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39m+\u001b[39m c_gate_mlp\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m context_ff_output\n\u001b[1;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m encoder_hidden_states, hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/diffusers/models/attention.py:840\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m     deprecate(\u001b[39m\"\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1.0.0\u001b[39m\u001b[39m\"\u001b[39m, deprecation_message)\n\u001b[1;32m    839\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet:\n\u001b[0;32m--> 840\u001b[0m     hidden_states \u001b[39m=\u001b[39m module(hidden_states)\n\u001b[1;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/diffusers/models/activations.py:88\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m---> 88\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(hidden_states)\n\u001b[1;32m     89\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(hidden_states)\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 49999\n",
    "for block_num in [6]:\n",
    "    # for index in [0, 250, 500, 750]:\n",
    "    for index in [500, 750, 250, 0]:\n",
    "        name = f'index_{index:03d}_block_{block_num}'\n",
    "        save_path = f'/mnt/bn/us-aigc-temp/henry/data/captions/dift/dift_{name}_step_{epoch}.json'\n",
    "        load_path = f'/mnt/bn/us-aigc-temp/henry/data/dift/{name}/epoch_0_step_{epoch}/'\n",
    "        print(f\"Loading pipeline from {load_path}\")\n",
    "        pipe = UniLatentPipeline.from_pretrained(load_path, torch_dtype=torch.float32)\n",
    "        assert torch.allclose(pipe.text_decoder.transformer.lm_head.weight, pipe.text_decoder.transformer.transformer.wte.weight)\n",
    "\n",
    "        pipe = prepare(accelerator, pipe)\n",
    "        print(f\"Running sampler for {name}:\")\n",
    "        sampler_kwargs = {'index': index, 'block_num': block_num}\n",
    "        generate_captions(pipe, dataloader, save_path, dift_sampler, sampler_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
